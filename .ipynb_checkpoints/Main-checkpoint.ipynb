{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \n",
    "    def get_file_path(self, file_name):\n",
    "        path = '/root/Data/IAM/Words'\n",
    "        destination = file_name[0]\n",
    "        path = os.path.join(path, destination)\n",
    "        for i in range(1, len(file_name)):\n",
    "            destination = destination + \"-\" + file_name[i]\n",
    "            path = os.path.join(path, destination)\n",
    "\n",
    "        path = path + '.png'\n",
    "        return (path)\n",
    "\n",
    "\n",
    "    def get_image_paths_labels(self):\n",
    "        img_paths = []\n",
    "        img_labels = []\n",
    "        data_path = '/root/Data/IAM'\n",
    "\n",
    "        words = os.path.join(data_path, 'words.txt')\n",
    "\n",
    "        with open(words) as labels_file:\n",
    "            for line in labels_file:\n",
    "                if line[0] == \"#\":\n",
    "                    continue\n",
    "                else:\n",
    "                    label = line.split(' ')[-1].strip('\\n')\n",
    "                    file_name = line.split(' ')[0].split('-')\n",
    "                    file_name[2] = file_name[2] + '-' + file_name[3]\n",
    "                    file_name.pop()\n",
    "\n",
    "                    path = self.get_file_path(file_name)\n",
    "                    img_paths.append(path)\n",
    "                    img_labels.append(label)\n",
    "\n",
    "        return (img_paths, img_labels)\n",
    "\n",
    "\n",
    "    def make_train_test(self):\n",
    "\n",
    "        img_paths, img_labels = self.get_image_paths_labels()\n",
    "\n",
    "        #print ('Total number of images {}'.format(len(img_paths)))\n",
    "        word_count = defaultdict(int)\n",
    "\n",
    "        for word in img_labels:\n",
    "            word_count[word] += 1\n",
    "\n",
    "        common_words_counts = Counter(word_count).most_common(20)\n",
    "\n",
    "        common_words = list(list(zip(*common_words_counts))[0])\n",
    "\n",
    "    #print (common_words)\n",
    "\n",
    "        not_common_paths_labels = []\n",
    "\n",
    "        for i, label in enumerate(img_labels):\n",
    "            if label not in common_words:\n",
    "                not_common_paths_labels.append((img_paths[i], label))\n",
    "\n",
    "    #print (not_common_paths_labels[1:10])\n",
    "\n",
    "        random.shuffle(not_common_paths_labels)\n",
    "\n",
    "        train_len = int(0.7 * len(not_common_paths_labels))\n",
    "        test_len = len(not_common_paths_labels) - train_len\n",
    "\n",
    "        train_paths_labels = not_common_paths_labels[0:train_len]\n",
    "        test_paths_labels = not_common_paths_labels[train_len:]\n",
    "\n",
    "        print ('Length of training data is {} test data is {}'.format(len(train_paths_labels), len(test_paths_labels)))\n",
    "        train_imgs_ts = list()\n",
    "        train_imgs_bs = list()\n",
    "        train_labels = list()\n",
    "        train_n = list()\n",
    "\n",
    "        count = 0\n",
    "        width, height = 128, 32\n",
    "\n",
    "        for path_label in train_paths_labels:\n",
    "#             count += 1\n",
    "#             if count == 10:\n",
    "#                 break\n",
    "\n",
    "            n = len(path_label[1])\n",
    "            img = cv2.imread(path_label[0])\n",
    "            if (img is not None) and (not np.isnan(img).any()):   #It is a valid image and the input does not contain empty values\n",
    "                img = (cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)) / 255.0\n",
    "                img_ts = cv2.resize(img, (width, height)).astype(np.float32).reshape(1, 32, 128, 1) \n",
    "                train_imgs_ts.append(img_ts)\n",
    "\n",
    "                img_bs = cv2.resize(img, (16*n, height)).astype(np.float32)\n",
    "                shape = img_bs.shape\n",
    "                img_bs = img_bs.reshape((1, shape[0], shape[1], 1))\n",
    "                train_imgs_bs.append(img_bs)\n",
    "\n",
    "                train_labels.append(path_label[1])\n",
    "                train_n.append(n)\n",
    "\n",
    "\n",
    "        train = {'imgs_ts':train_imgs_ts, 'imgs_bs':train_imgs_bs, 'labels':train_labels, 'n':train_n}\n",
    "\n",
    "        test_imgs_ts = list()\n",
    "        test_imgs_bs = list()\n",
    "        test_labels = list()\n",
    "        test_n = list()\n",
    "\n",
    "        count = 0\n",
    "        width, height = 128, 32\n",
    "\n",
    "        for path_label in test_paths_labels:\n",
    "#             count += 1\n",
    "#             if count == 2:\n",
    "#                 break\n",
    "\n",
    "            n = len(path_label[1])\n",
    "            img = cv2.imread(path_label[0])\n",
    "            if img is not None:\n",
    "                img = (cv2.cvtColor(img,cv2.COLOR_BGR2GRAY))/255.0\n",
    "\n",
    "                img_ts = cv2.resize(img, (width, height)).astype(np.float32).reshape(1, 32, 128, 1)\n",
    "                test_imgs_ts.append(img_ts)\n",
    "\n",
    "                img_bs = cv2.resize(img, (16*n, height)).astype(np.float32)\n",
    "                shape = img_bs.shape\n",
    "                img_bs = img_bs.reshape((1, shape[0], shape[1], 1))\n",
    "                test_imgs_bs.append(img_bs)\n",
    "\n",
    "                test_labels.append(path_label[1])\n",
    "                test_n.append(n)\n",
    "\n",
    "\n",
    "        test = {'imgs_ts':test_imgs_ts, 'imgs_bs':test_imgs_bs, 'labels':test_labels, 'n':test_n}\n",
    "\n",
    "        return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Models:\n",
    "    \n",
    "    def get_top_stream(self):\n",
    "        input_shape = (32, 128, 1)\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), input_shape=input_shape, padding='same'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "        model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "        model.add(layers.Conv2D(filters=256, kernel_size=(3, 3), padding='same'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(1024))\n",
    "        return (model)\n",
    "\n",
    "\n",
    "    def get_bottom_stream(self):\n",
    "\n",
    "        model = None\n",
    "        model = tf.keras.Sequential()\n",
    "        input_shape = (32, None, 1)\n",
    "        model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), input_shape=input_shape, padding='same'))\n",
    "        model.add(layers.Conv2D(filters=128, kernel_size=(3, 3),  padding='same'))\n",
    "        model.add(layers.Conv2D(filters=128, kernel_size=(3, 3),  padding='same'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "        model.add(layers.Conv2D(filters=256, kernel_size=(3, 3),  padding='same'))\n",
    "        model.add(layers.Conv2D(filters=256, kernel_size=(3, 3),  padding='same'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "        model.add(layers.Conv2D(filters=512, kernel_size=(3, 3),  padding='same'))\n",
    "        model.add(layers.Conv2D(filters=512, kernel_size=(3, 3),  padding='same'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "        padding = [[0, 0], [0, 0], [2, 2], [0, 0]]   \n",
    "        model.add(layers.Conv2D(filters=1024, kernel_size=(4, 4), padding=padding))\n",
    "\n",
    "        return (model)\n",
    "\n",
    "    def get_middle_stream(self, Ns):\n",
    "        model = None\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(layers.Dense(Ns, input_shape = (None, None, 1024), activation=tf.nn.relu))\n",
    "        return (model)\n",
    "\n",
    "    def get_character_error_rate(self, word1, word2):\n",
    "        rows = len(word1) + 1\n",
    "        cols = len(word2) + 1\n",
    "        error_matrix = [[0 for i in range(cols)] for j in range(rows)]\n",
    "\n",
    "        for i in range(cols):\n",
    "            error_matrix[0][i] = i\n",
    "\n",
    "        for i in range(rows):\n",
    "            error_matrix[i][0] = i\n",
    "\n",
    "        for i in range(1, rows):\n",
    "            for j in range(1, cols):\n",
    "                a = error_matrix[i-1][j] + 1\n",
    "                b = error_matrix[i][j-1] + 1\n",
    "                if word1[i-1] == word2[j-1]:\n",
    "                    c = error_matrix[i-1][j-1]\n",
    "                else:\n",
    "                    c = error_matrix[i-1][j-1] + 2\n",
    "                error_matrix[i][j] = min(a, b, c)\n",
    "\n",
    "        return (error_matrix[rows-1][cols-1])\n",
    "\n",
    "#     def norm_loss(self, out, target):\n",
    "#         return (tf.norm(target - out))\n",
    "\n",
    "#     def norm_1_loss(self, out, target):\n",
    "#         return (tf.norm(target - out, 1))\n",
    "    \n",
    "    def norm_inf_loss(self, out, target):\n",
    "        return (tf.norm(target - out, np.inf))\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        return (optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ts_model, ms_model, bs_model, train_data, optimizer):\n",
    "    f = open('train.LOG', 'a')\n",
    "    f.write('In training\\n')\n",
    "    f.close()\n",
    "        \n",
    "    save_path = '/root/arun/Handwriting-Recognition/models'\n",
    "\n",
    "    _, labels_pos_map, pos_labels_map = find_unique_characters()\n",
    "\n",
    "    epochs = 10\n",
    "    train_imgs_ts, train_imgs_bs, train_labels, train_n  =\\\n",
    "         train_data['imgs_ts'], train_data['imgs_bs'], train_data['labels'], train_data['n']\n",
    "    try:\n",
    "        for epoch in range(1, epochs+1):\n",
    "            f = open('train.LOG', 'a')\n",
    "            f.write('Epoch is {}\\n'.format(epoch))\n",
    "            f.close()\n",
    "\n",
    "            epoch_loss = 0\n",
    "            cer = 0\n",
    "            indexes = np.arange(0, len(train_imgs_ts), 1)\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "            for i, idx in enumerate(indexes):\n",
    "                if (i%10000 == 0):\n",
    "                    f = open('train.LOG', 'a')\n",
    "                    f.write('Epoch {} i {}   '.format(epoch, i))\n",
    "                    f.close()\n",
    "\n",
    "                img_ts = tf.convert_to_tensor(train_imgs_ts[idx])  #top stream input\n",
    "                img_bs = tf.convert_to_tensor(train_imgs_bs[idx])  # bottom stream input\n",
    "                actual_word = train_labels[idx]\n",
    "\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                # ms_model - middle stream model, ts - top stream model, bs_model - bottom stream model\n",
    "                    out = ms_model(ts_model(img_ts) + bs_model(img_bs))\n",
    "                    out = tf.reshape(out, (out.shape[2], out.shape[3]))\n",
    "                    out = out / tf.norm(out, axis=1, keepdims=True)\n",
    "                    predicted_word = list()\n",
    "                    for i in range(1, len(out), 2):\n",
    "                        predicted_word.append(pos_labels_map[np.argmax(out[i])])\n",
    "                    predicted_word = ''.join(predicted_word)\n",
    "\n",
    "                # loss = get_character_error_rate(actual_word, predicted_word)    ->  Not able to construct gradients using this method\n",
    "                    target = np.zeros(out.shape)\n",
    "                    for i in range(1, out.shape[0], 2):\n",
    "                        char = actual_word[(i - 1)//2]  # Shape of out is 2n\n",
    "                        pos = labels_pos_map[char]\n",
    "                        target[i][pos] = 1.0\n",
    "                    target = tf.convert_to_tensor(target, dtype=tf.float32)\n",
    "\n",
    "                    loss = models.norm_inf_loss(target, out)\n",
    "\n",
    "                grads_ms = tape.gradient(loss, ms_model.trainable_weights)\n",
    "                grads_ts = tape.gradient(loss, ts_model.trainable_weights)\n",
    "                grads_bs = tape.gradient(loss, bs_model.trainable_weights)\n",
    "\n",
    "                grads_ms, _ = tf.clip_by_global_norm(grads_ms, 1.0)\n",
    "                grads_ts, _ = tf.clip_by_global_norm(grads_ts, 1.0)\n",
    "                grads_bs, _ = tf.clip_by_global_norm(grads_bs, 1.0)\n",
    "\n",
    "                grads_ms = adjust_gradient(grads_ms, epoch)\n",
    "                grads_ts = adjust_gradient(grads_ts, epoch)\n",
    "                grads_bs = adjust_gradient(grads_bs, epoch)\n",
    "\n",
    "                optimizer.apply_gradients(zip(grads_ms, ms_model.trainable_weights))\n",
    "                optimizer.apply_gradients(zip(grads_bs, bs_model.trainable_weights))\n",
    "                optimizer.apply_gradients(zip(grads_ts, ts_model.trainable_weights))\n",
    "\n",
    "                epoch_loss += loss\n",
    "                cer += models.get_character_error_rate(actual_word, predicted_word)\n",
    "\n",
    "                if(i + 1 % 10000 == 0):\n",
    "                    print (actual_word, predicted_word, sep=' ', end='    ')\n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    ts_model.save(os.path.join(save_path, 'ts_model.h5'))\n",
    "                    ms_model.save(os.path.join(save_path, 'ms_model.h5'))\n",
    "                    bs_model.save(os.path.join(save_path, 'bs_model.h5'))\n",
    "\n",
    "            try:        \n",
    "                epoch_loss = int(epoch_loss)\n",
    "                print ('Epoch {} Loss {}  CER {}\\n'.format(epoch, epoch_loss, cer))\n",
    "            except:\n",
    "                print ('Error in conversion {} {}'.format(epoch, epoch_loss))\n",
    "                print (loss)\n",
    "                return (ts_model, ms_model, bs_model)\n",
    "        \n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        ts_model.save(os.path.join(save_path, 'ts_model.h5'))\n",
    "        ms_model.save(os.path.join(save_path, 'ms_model.h5'))\n",
    "        bs_model.save(os.path.join(save_path, 'bs_model.h5'))\n",
    "        \n",
    "    return (ts_model, ms_model, bs_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ts_model, ms_model, bs_model, test_data):\n",
    "    _, labels_pos_map, pos_labels_map = find_unique_characters()\n",
    "\n",
    "    test_imgs_ts, test_imgs_bs, test_labels, test_n =\\\n",
    "         test_data['imgs_ts'], test_data['imgs_bs'], test_data['labels'], test_data['n']\n",
    "\n",
    "    total_characters = 0\n",
    "    correct_prediction = 0\n",
    "\n",
    "    for idx in range(len(test_imgs_ts)):\n",
    "        img_ts = tf.convert_to_tensor(test_imgs_ts[idx])  #top stream input\n",
    "        img_bs = tf.convert_to_tensor(test_imgs_bs[idx])  # bottom stream input\n",
    "        actual_word = test_labels[idx]\n",
    "\n",
    "        out = ms_model(ts_model(img_ts) + bs_model(img_bs))\n",
    "        out = tf.reshape(out, (out.shape[2], out.shape[3]))\n",
    "        out = out / tf.norm(out, axis=1, keepdims=True)\n",
    "        predicted_word = list()\n",
    "        for i in range(1, len(out), 2):\n",
    "            predicted_word.append(pos_labels_map[np.argmax(out[i])])\n",
    "        predicted_word = ''.join(predicted_word)\n",
    "\n",
    "        total_characters += len(actual_word)\n",
    "        for i in range(len(actual_word)):\n",
    "            if actual_word[i] == predicted_word[i]:\n",
    "                correct_prediction += 1\n",
    "\n",
    "        if (idx < 5):\n",
    "            print (actual_word, predicted_word, sep=' ', end='   ')\n",
    "\n",
    "    print ('Total characters {} correct prediction {}'.format(total_characters, correct_prediction))\n",
    "    print (\"Character wise accuracy {}\".format(correct_prediction*100 / total_characters))\n",
    "    return (0)\n",
    "\n",
    "def find_unique_characters():\n",
    "#Finding Ns and creating a map of labels and index\n",
    "    _, labels = dataloader.get_image_paths_labels()\n",
    "    unique_characters = list()\n",
    "    for i, label in enumerate(labels):\n",
    "        for char in label:\n",
    "            if char not in unique_characters:\n",
    "                unique_characters.append(char)\n",
    "\n",
    "    unique_characters.sort()\n",
    "    Ns = len(unique_characters)\n",
    "    \n",
    "    pos_labels_map = {}\n",
    "    labels_pos_map = {}\n",
    "    for i, char in enumerate(unique_characters):\n",
    "        labels_pos_map[char] = i\n",
    "        pos_labels_map[i] = char\n",
    "\n",
    "    return (Ns, labels_pos_map, pos_labels_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_gradient(grads, epoch, eta=0.03, gamma=0.55):\n",
    "    #Adding gradient noise and clipping the gradients\n",
    "    var = eta / ((1 + epoch) ** gamma)\n",
    "    for i, grad in enumerate(grads):\n",
    "        grads[i] = grad + np.random.normal(0, var, grad.shape)\n",
    "    return (grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data is 52850 test data is 22650\n",
      "Total samples in training data is 52848 and in test data is 22650\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    Ns, _, __ = find_unique_characters()\n",
    "    \n",
    "    start = datetime.now()\n",
    "    train_data, test_data = dataloader.make_train_test()\n",
    "    end = datetime.now()\n",
    "    \n",
    "    f = open('main.LOG', 'a')\n",
    "    f.write('Time taken to load data is {} seconds'.format((end - start).seconds))\n",
    "    f.close()\n",
    "    \n",
    "    train_count = len(train_data['n'])\n",
    "    test_count = len(test_data['n'])\n",
    "    print ('Total samples in training data is {} and in test data is {}'.format(train_count, test_count))\n",
    "\n",
    "    models_loc = '/root/arun/Handwriting-Recognition/models'\n",
    "\n",
    "    # files = os.listdir(models_loc)\n",
    "    # for file in files:\n",
    "    #     os.remove(os.path.join(models_loc, file))\n",
    "    \n",
    "    if len(os.listdir(models_loc)) >= 3:\n",
    "        ts_model = tf.keras.models.load_model(os.path.join(models_loc, 'ts_model.h5'))\n",
    "        bs_model = tf.keras.models.load_model(os.path.join(models_loc, 'bs_model.h5'))\n",
    "        ms_model = tf.keras.models.load_model(os.path.join(models_loc, 'ms_model.h5'))\n",
    "    else:\n",
    "        ts_model = models.get_top_stream()\n",
    "        bs_model = models.get_bottom_stream()\n",
    "        ms_model = models.get_middle_stream(Ns)\n",
    "        \n",
    "    optimizer = models.get_optimizer()\n",
    "\n",
    "    start = datetime.now()\n",
    "    ts_model, ms_model, bs_model = train(ts_model, ms_model, bs_model, train_data, optimizer)\n",
    "    end = datetime.now()\n",
    "    f = open('main.LOG', 'a')\n",
    "    f.write('Time trained {} seconds'.format((end - start).seconds))\n",
    "    f.close()\n",
    "    test(ts_model, ms_model, bs_model, test_data)\n",
    "#Training model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    models = Models()\n",
    "    dataloader = DataLoader()\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.13% accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGD**\n",
    "\n",
    "    lr = 0.01 24 epochs, CER = . Test accuracy: 8%.\n",
    "    lr = 0.0001 Problem: Very slow. Got stock at loss=876. After that, there was no reduction in loss value.\n",
    "    lr = 0.001 Problem: Very slow. Epoch: 30 loss 326 CER 856. Updates are slow.\n",
    "    \n",
    "**Adagrad**\n",
    "\n",
    "    lr = 0.01\n",
    "\n",
    "**RMSProp**\n",
    "\n",
    "    lr = 0.01\n",
    "    lr = 0.001\n",
    "    \n",
    "**Adam**\n",
    "\n",
    "    lr = 0.1 norm_loss Accuracy: 13.15%, CER: 756\n",
    "    lr = 0.1 norm_1_loss Accuracy: , CER: 1134\n",
    "    lr = 0.1 norm_inf_loss Accuracy: , CER:1024\n",
    "    \n",
    "    lr = 0.01 norm_inf_loss, Test Accuracy: 25.53%, CER: 198\n",
    "    lr = 0.01 norm_loss NaN grads\n",
    "    lr = 0.01 norm_1_loss Bad convergence\n",
    "    \n",
    "    lr = 0.001 norm_1_loss No convergence observed.\n",
    "    lr = 0.001 norm_loss  NaN grads\n",
    "    lr = 0.001 norm_inf_loss CER: 46, Accuracy: 12.9%\n",
    "    \n",
    "    lr = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy loss\n",
    "\n",
    "Next character prediction.\n",
    "\n",
    "GeLU activation function.\n",
    "\n",
    "Using a dictionary to make correct words.\n",
    "\n",
    "Represent characters as numerical encodings.\n",
    "\n",
    "https://arxiv.org/pdf/1706.03762.pdf Refer section 5.3 Optimizer.\n",
    "\n",
    "https://arxiv.org/pdf/1807.01544.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1507.05717v1.pdf\n",
    "\n",
    "We could try a recurrent model which takes the adjacent cells as well for input. Make it as an end to end model - given a photo copy of a page, it extracts the text from it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
